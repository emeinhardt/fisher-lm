{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eric Meinhardt / emeinhardt@ucsd.edu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.580193Z",
     "start_time": "2019-08-23T20:33:54.577100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prints **all** console output, not just last item in cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.686787Z",
     "start_time": "2019-08-23T20:33:54.581759Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.692718Z",
     "start_time": "2019-08-23T20:33:54.688488Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.700142Z",
     "start_time": "2019-08-23T20:33:54.694286Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.710695Z",
     "start_time": "2019-08-23T20:33:54.701329Z"
    }
   },
   "outputs": [],
   "source": [
    "def readStringList(fp):\n",
    "    lines = []\n",
    "    with open(fp, 'r') as file:\n",
    "        for line in file:\n",
    "            lines.append(line.rstrip())\n",
    "    return lines\n",
    "\n",
    "def writeStringList(fp, strings):\n",
    "    with open(fp, 'w') as file:\n",
    "        strings_w_linebreaks = list(map(lambda l: l + \"\\n\", strings))\n",
    "        file.writelines(strings_w_linebreaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.716359Z",
     "start_time": "2019-08-23T20:33:54.711936Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.724180Z",
     "start_time": "2019-08-23T20:33:54.718192Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.835285Z",
     "start_time": "2019-08-23T20:33:54.725673Z"
    }
   },
   "outputs": [],
   "source": [
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:54.839448Z",
     "start_time": "2019-08-23T20:33:54.836845Z"
    }
   },
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "# from itertools import chain\n",
    "# import re\n",
    "# from more_itertools import replace\n",
    "# from funcy import compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:33:55.494229Z",
     "start_time": "2019-08-23T20:33:54.840799Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "J = -1\n",
    "BACKEND = 'multiprocessing'\n",
    "# BACKEND = 'loky'\n",
    "V = 10\n",
    "PREFER = 'processes'\n",
    "# PREFER = 'threads'\n",
    "\n",
    "def par(gen_expr):\n",
    "    return Parallel(n_jobs=J, backend=BACKEND, verbose=V, prefer=PREFER)(gen_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:09.824547Z",
     "start_time": "2019-08-23T20:33:55.495699Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:09.827861Z",
     "start_time": "2019-08-23T20:34:09.825858Z"
    }
   },
   "outputs": [],
   "source": [
    "repo_dir = '/mnt/cube/home/AD/emeinhar/fisher-lm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview-&amp;-Requirements\" data-toc-modified-id=\"Overview-&amp;-Requirements-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview &amp; Requirements</a></span></li><li><span><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading data</a></span></li><li><span><a href=\"#Splitting-the-corpus-into-training-and-test-sets\" data-toc-modified-id=\"Splitting-the-corpus-into-training-and-test-sets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Splitting the corpus into training and test sets</a></span></li><li><span><a href=\"#Build-preliminary-files-for-each-language-model-of-interest\" data-toc-modified-id=\"Build-preliminary-files-for-each-language-model-of-interest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build preliminary files for each language model of interest</a></span></li><li><span><a href=\"#Creating-and-querying-a-model-using-the-kenlm-python-package\" data-toc-modified-id=\"Creating-and-querying-a-model-using-the-kenlm-python-package-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Creating and querying a model using the <code>kenlm</code> python package</a></span></li><li><span><a href=\"#Calculate-perplexity-of-the-test-set-for-each-model\" data-toc-modified-id=\"Calculate-perplexity-of-the-test-set-for-each-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Calculate perplexity of the test set for each model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview & Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:09.922096Z",
     "start_time": "2019-08-23T20:34:09.829018Z"
    }
   },
   "outputs": [],
   "source": [
    "#FXIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:09.927396Z",
     "start_time": "2019-08-23T20:34:09.923579Z"
    }
   },
   "outputs": [],
   "source": [
    "main_corpus_fn = 'fisher_utterances_main.txt'\n",
    "bbn_corpus_fn = 'fisher_utterances_bbn.txt'\n",
    "\n",
    "my_corpus_fn = main_corpus_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:10.479945Z",
     "start_time": "2019-08-23T20:34:09.928885Z"
    }
   },
   "outputs": [],
   "source": [
    "utterances = readStringList(my_corpus_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:10.484855Z",
     "start_time": "2019-08-23T20:34:10.481327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1077813"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:11.607956Z",
     "start_time": "2019-08-23T20:34:10.485725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i mean no money is very important definitely and a million dollars is a dream come true for me i mean'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'mean i me for true come dream a is dollars million a and definitely important very is money no mean i'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[0]\n",
    "\n",
    "def reverse_utterance(u):\n",
    "    ws = u.split(' ')\n",
    "    rev = list(reversed(ws))\n",
    "    rev_str = ' '.join(rev)\n",
    "    return rev_str\n",
    "\n",
    "reverse_utterance(utterances[0])\n",
    "\n",
    "utterances_reversed = list(map(reverse_utterance, utterances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the corpus into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:11.612322Z",
     "start_time": "2019-08-23T20:34:11.609334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107781"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_proportion = 0.10\n",
    "\n",
    "exact_num_test_utterances = round(test_proportion * len(utterances))\n",
    "exact_num_test_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:11.618629Z",
     "start_time": "2019-08-23T20:34:11.614490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i mean no money is very important definitely and a million dollars is a dream come true for me i mean'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"and they're not as strict either\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[0]\n",
    "utterances[23124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:12.801099Z",
     "start_time": "2019-08-23T20:34:11.620069Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = list(range(len(utterances)))\n",
    "\n",
    "shuffled_indices = deepcopy(indices)\n",
    "random.shuffle(shuffled_indices) #stateful, in-place shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:14.466564Z",
     "start_time": "2019-08-23T20:34:12.802178Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_utterances = list(map(lambda idx: utterances[idx],\n",
    "                               shuffled_indices))\n",
    "shuffled_utterances_reversed = list(map(reverse_utterance, shuffled_utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:14.472740Z",
     "start_time": "2019-08-23T20:34:14.467650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so that ah that ah yes yes yeah and so i actually feel safe about it i mean i don't know if it is a\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'yeah'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"a is it if know don't i mean i it about safe feel actually i so and yeah yes yes ah that ah that so\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'yeah'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_utterances[0]\n",
    "shuffled_utterances[23124]\n",
    "\n",
    "shuffled_utterances_reversed[0]\n",
    "shuffled_utterances_reversed[23124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:14.515905Z",
     "start_time": "2019-08-23T20:34:14.473640Z"
    }
   },
   "outputs": [],
   "source": [
    "test_utterances = shuffled_utterances[:exact_num_test_utterances]\n",
    "training_utterances = shuffled_utterances[exact_num_test_utterances:]\n",
    "\n",
    "test_utterances_reversed = shuffled_utterances_reversed[:exact_num_test_utterances]\n",
    "training_utterances_reversed = shuffled_utterances_reversed[exact_num_test_utterances:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, we want to export these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:14.522246Z",
     "start_time": "2019-08-23T20:34:14.517001Z"
    }
   },
   "outputs": [],
   "source": [
    "test_set_prefix = 'fisher_test_utterances'\n",
    "training_set_prefix = 'fisher_training_utterances'\n",
    "\n",
    "test_set_fn = test_set_prefix + '.txt'\n",
    "training_set_fn = training_set_prefix + '.txt'\n",
    "\n",
    "test_set_rev_fn = test_set_prefix + '_rev' + '.txt'\n",
    "training_set_rev_fn = training_set_prefix + '_rev' + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:16.588112Z",
     "start_time": "2019-08-23T20:34:14.523160Z"
    }
   },
   "outputs": [],
   "source": [
    "writeStringList(test_set_fn, test_utterances)\n",
    "writeStringList(training_set_fn, training_utterances)\n",
    "\n",
    "writeStringList(test_set_rev_fn, test_utterances_reversed)\n",
    "writeStringList(training_set_rev_fn, training_utterances_reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:16.718877Z",
     "start_time": "2019-08-23T20:34:16.589170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tyou know\r\n",
      "     2\toh yeah\r\n",
      "     3\tand the child the child may not or it'll lose even if it remembers it it may it may not have it may hamper it's ability to um to choose how to approach\r\n",
      "     4\tsome people in her unit are getting called up she hasn't yet\r\n",
      "     5\twas she a nice lady\r\n",
      "     6\tyes that if you if\r\n",
      "     7\thi i'm mark king\r\n",
      "     8\tyeah i guess that's it's the best place to get lobster isn't it maine lobster i've heard that's 'bout the best\r\n",
      "     9\twell i guess ah it's popular\r\n",
      "    10\twhat's that\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "%cat -n /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:16.851597Z",
     "start_time": "2019-08-23T20:34:16.720960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tknow you\r\n",
      "     2\tyeah oh\r\n",
      "     3\tapproach to how choose to um to ability it's hamper may it have not may it may it it remembers it if even lose it'll or not may child the child the and\r\n",
      "     4\tyet hasn't she up called getting are unit her in people some\r\n",
      "     5\tlady nice a she was\r\n",
      "     6\tif you if that yes\r\n",
      "     7\tking mark i'm hi\r\n",
      "     8\tbest the 'bout that's heard i've lobster maine it isn't lobster get to place best the it's that's guess i yeah\r\n",
      "     9\tpopular it's ah guess i well\r\n",
      "    10\tthat what's\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "%cat -n /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances_rev.txt | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build preliminary files for each language model of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as we are concerned, there are two parameter choices for the language model:\n",
    " - the choice of $n$ (as in $n$-gram)\n",
    " - the minimum token count threshold $p$ tokens must have before they are pruned\n",
    "\n",
    "Choices:\n",
    " - $n \\in \\{1, 2, 3, 4, 5\\}$\n",
    " - $p \\in \\{0, 5, 10\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:16.856355Z",
     "start_time": "2019-08-23T20:34:16.853372Z"
    }
   },
   "outputs": [],
   "source": [
    "N = (1,2,3,4,5)\n",
    "P = (0,5,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two shell commands below \n",
    " - build a unigram model (from the complete set of LDC transcriptions) with no pruning.\n",
    " - build a binary memory map version of that same `.arpa` file for faster queries.\n",
    " \n",
    "```\n",
    "/home/AD/emeinhar/GitHub/kenlm/build/bin/lmplz -o 1 --text fisher_training_utterances.txt --arpa fisher_training_utterances_1gram.arpa\n",
    "```\n",
    "\n",
    "```\n",
    "/home/AD/emeinhar/GitHub/kenlm/build/bin/build_binary fisher_training_utterances_1gram.arpa fisher_training_utterances_1gram.mmap\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `subprocess` module to build and execute as many of these shell calls as we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:16.863025Z",
     "start_time": "2019-08-23T20:34:16.857747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cube/home/AD/emeinhar/fisher-lm'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:16.869673Z",
     "start_time": "2019-08-23T20:34:16.864212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/GitHub/kenlm/build/bin/lmplz'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/home/AD/emeinhar/GitHub/kenlm/build/bin/build_binary'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kenlm_path = '/home/AD/emeinhar/GitHub/kenlm/'\n",
    "lmplz_path = os.path.join(kenlm_path, 'build/bin/lmplz')\n",
    "lmplz_path\n",
    "lmplz = lmplz_path\n",
    "\n",
    "build_binary_path = os.path.join(kenlm_path, 'build/bin/build_binary')\n",
    "build_binary_path\n",
    "build_binary = build_binary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:41:06.426311Z",
     "start_time": "2019-08-23T20:41:06.416608Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(n, p, training_set_prefix, rev=False):\n",
    "    assert n in N\n",
    "    assert p in P\n",
    "    assert p == 0, 'p other than 0 not supported yet'\n",
    "    assert rev in {True, False}\n",
    "    \n",
    "    print(\"Building .arpa and .memmap files for n='{0}', p='{1}', training_set_prefix='{2}', rev='{3}'\".format(n, p, training_set_prefix, rev))\n",
    "    print('\\n')\n",
    "    \n",
    "    rev_str = '' if not rev else '_rev'\n",
    "    training_set_fn = training_set_prefix + rev_str + '.txt'\n",
    "    arpa_fn = training_set_prefix + rev_str + '_{0}gram'.format(n) + '.arpa'\n",
    "    mmap_fn = training_set_prefix + rev_str + '_{0}gram'.format(n) + '.mmap'\n",
    "    \n",
    "    fns = {'training_set':training_set_fn,\n",
    "           'arpa':arpa_fn,\n",
    "           'mmap':mmap_fn}\n",
    "    \n",
    "    build_arpa_file = [lmplz, '-o', str(n), \n",
    "                              '--text', training_set_fn,\n",
    "                              '--arpa', arpa_fn]\n",
    "    build_mmap_file = [build_binary, arpa_fn, \n",
    "                                     mmap_fn]\n",
    "\n",
    "#     subprocess.run(build_arpa_file)\n",
    "#     subprocess.run(build_mmap_file)\n",
    "#     arpa_build_out = subprocess.run(build_arpa_file, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "#     print(arpa_build_out)\n",
    "    \n",
    "#     binary_build_out = subprocess.run(build_mmap_file, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "#     print(binary_build_out)\n",
    "    \n",
    "    build_arpa_file_cmd = ' '.join(build_arpa_file)\n",
    "    build_mmap_file_cmd = ' '.join(build_mmap_file)\n",
    "    \n",
    "#     os.system(build_arpa_file_cmd)\n",
    "#     os.system(build_mmap_file_cmd)\n",
    "#     arpa_build_out = subprocess.run(build_arpa_file_cmd, shell=True, stdout=subprocess.PIPE, \n",
    "#                         universal_newlines=True)\n",
    "    arpa_build_out = subprocess.getoutput(build_arpa_file_cmd)\n",
    "    print(arpa_build_out)\n",
    "    \n",
    "    print(' ')\n",
    "    if n == 1:\n",
    "        print('build_binary requires n > 1. Skipping.\\n')\n",
    "        fns['mmap'] = None\n",
    "    if n != 1:\n",
    "    #     binary_build_out = subprocess.run(build_mmap_file_cmd, shell=True, stdout=subprocess.PIPE, \n",
    "    #                         universal_newlines=True)\n",
    "        binary_build_out = subprocess.getoutput(build_mmap_file_cmd)\n",
    "        print(binary_build_out)\n",
    "        print('\\n')\n",
    "    \n",
    "    print('Done.')\n",
    "    print('\\n')\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:34:17.080974Z",
     "start_time": "2019-08-23T20:34:17.075040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0, 'fisher_training_utterances', False),\n",
       " (1, 0, 'fisher_training_utterances', True),\n",
       " (2, 0, 'fisher_training_utterances', False),\n",
       " (2, 0, 'fisher_training_utterances', True),\n",
       " (3, 0, 'fisher_training_utterances', False),\n",
       " (3, 0, 'fisher_training_utterances', True),\n",
       " (4, 0, 'fisher_training_utterances', False),\n",
       " (4, 0, 'fisher_training_utterances', True),\n",
       " (5, 0, 'fisher_training_utterances', False),\n",
       " (5, 0, 'fisher_training_utterances', True))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_combinations = tuple(product(N, \n",
    "                                       {0}, #P,\n",
    "                                       {training_set_prefix},\n",
    "                                       {False, True},\n",
    "                                      ))\n",
    "parameter_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.362479Z",
     "start_time": "2019-08-23T20:41:20.172914Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building .arpa and .memmap files for n='1', p='0', training_set_prefix='fisher_training_utterances', rev='False'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496\n",
      "Statistics:\n",
      "1 42458 D1=0.560943 D2=1.00505 D3+=1.49408\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 1824 assuming -p 1.5\n",
      "probing 1990 assuming -r models -p 1.5\n",
      "trie    1238 without quantization\n",
      "trie    1118 assuming -q 8 -b 8 quantization \n",
      "trie    1238 assuming -a 22 array pointer compression\n",
      "trie    1118 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760580 kB\tVmRSS:7284 kB\tRSSMax:35211872 kB\tuser:2.70723\tsys:6.04135\tCPU:8.74859\treal:8.78954\n",
      " \n",
      "build_binary requires n > 1. Skipping.\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='1', p='0', training_set_prefix='fisher_training_utterances', rev='True'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances_rev.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496\n",
      "Statistics:\n",
      "1 42458 D1=0.560943 D2=1.00505 D3+=1.49408\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 1824 assuming -p 1.5\n",
      "probing 1990 assuming -r models -p 1.5\n",
      "trie    1238 without quantization\n",
      "trie    1118 assuming -q 8 -b 8 quantization \n",
      "trie    1238 assuming -a 22 array pointer compression\n",
      "trie    1118 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760580 kB\tVmRSS:7288 kB\tRSSMax:35211632 kB\tuser:2.51134\tsys:6.11665\tCPU:8.62802\treal:8.67398\n",
      " \n",
      "build_binary requires n > 1. Skipping.\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='2', p='0', training_set_prefix='fisher_training_utterances', rev='False'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:108102172672\n",
      "Statistics:\n",
      "1 42458 D1=0.568635 D2=0.994079 D3+=1.51652\n",
      "2 861871 D1=0.68842 D2=1.06607 D3+=1.40606\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 16228 assuming -p 1.5\n",
      "probing 16393 assuming -r models -p 1.5\n",
      "trie     5939 without quantization\n",
      "trie     3521 assuming -q 8 -b 8 quantization \n",
      "trie     5939 assuming -a 22 array pointer compression\n",
      "trie     3521 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760576 kB\tVmRSS:21028 kB\tRSSMax:28827660 kB\tuser:3.70826\tsys:5.13728\tCPU:8.84556\treal:9.04037\n",
      " \n",
      "Reading fisher_training_utterances_2gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='2', p='0', training_set_prefix='fisher_training_utterances', rev='True'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances_rev.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:108102172672\n",
      "Statistics:\n",
      "1 42458 D1=0.562873 D2=1.02199 D3+=1.47276\n",
      "2 861871 D1=0.68842 D2=1.06607 D3+=1.40606\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 16228 assuming -p 1.5\n",
      "probing 16393 assuming -r models -p 1.5\n",
      "trie     5939 without quantization\n",
      "trie     3521 assuming -q 8 -b 8 quantization \n",
      "trie     5939 assuming -a 22 array pointer compression\n",
      "trie     3521 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760576 kB\tVmRSS:21300 kB\tRSSMax:28827584 kB\tuser:3.77488\tsys:5.09449\tCPU:8.86939\treal:9.12911\n",
      " \n",
      "Reading fisher_training_utterances_rev_2gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='3', p='0', training_set_prefix='fisher_training_utterances', rev='False'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:37600755712 3:70501416960\n",
      "Statistics:\n",
      "1 42458 D1=0.568635 D2=0.994079 D3+=1.51652\n",
      "2 861871 D1=0.701037 D2=1.08009 D3+=1.40893\n",
      "3 3259120 D1=0.783625 D2=1.0927 D3+=1.34433\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 76 assuming -p 1.5\n",
      "probing 81 assuming -r models -p 1.5\n",
      "trie    29 without quantization\n",
      "trie    15 assuming -q 8 -b 8 quantization \n",
      "trie    28 assuming -a 22 array pointer compression\n",
      "trie    14 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760580 kB\tVmRSS:21452 kB\tRSSMax:24450212 kB\tuser:6.01668\tsys:4.90824\tCPU:10.9249\treal:11.6635\n",
      " \n",
      "Reading fisher_training_utterances_3gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='3', p='0', training_set_prefix='fisher_training_utterances', rev='True'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances_rev.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:37600755712 3:70501416960\n",
      "Statistics:\n",
      "1 42458 D1=0.562873 D2=1.02199 D3+=1.47276\n",
      "2 861871 D1=0.697381 D2=1.06969 D3+=1.41492\n",
      "3 3259120 D1=0.783625 D2=1.0927 D3+=1.34433\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 76 assuming -p 1.5\n",
      "probing 81 assuming -r models -p 1.5\n",
      "trie    29 without quantization\n",
      "trie    15 assuming -q 8 -b 8 quantization \n",
      "trie    28 assuming -a 22 array pointer compression\n",
      "trie    14 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760580 kB\tVmRSS:21164 kB\tRSSMax:24450344 kB\tuser:6.26414\tsys:4.80011\tCPU:11.0643\treal:11.7878\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading fisher_training_utterances_rev_3gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='4', p='0', training_set_prefix='fisher_training_utterances', rev='False'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:18400368640 3:34500694016 4:55201107968\n",
      "Statistics:\n",
      "1 42458 D1=0.568635 D2=0.994079 D3+=1.51652\n",
      "2 861871 D1=0.701037 D2=1.08009 D3+=1.40893\n",
      "3 3259120 D1=0.797312 D2=1.11694 D3+=1.37379\n",
      "4 5760906 D1=0.867448 D2=1.16471 D3+=1.31979\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 194 assuming -p 1.5\n",
      "probing 218 assuming -r models -p 1.5\n",
      "trie     83 without quantization\n",
      "trie     44 assuming -q 8 -b 8 quantization \n",
      "trie     75 assuming -a 22 array pointer compression\n",
      "trie     37 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760572 kB\tVmRSS:21380 kB\tRSSMax:21275864 kB\tuser:9.03235\tsys:5.12502\tCPU:14.1574\treal:15.9718\n",
      " \n",
      "Reading fisher_training_utterances_4gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='4', p='0', training_set_prefix='fisher_training_utterances', rev='True'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances_rev.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:18400368640 3:34500694016 4:55201107968\n",
      "Statistics:\n",
      "1 42458 D1=0.562873 D2=1.02199 D3+=1.47276\n",
      "2 861871 D1=0.697381 D2=1.06969 D3+=1.41492\n",
      "3 3259120 D1=0.793798 D2=1.10843 D3+=1.35036\n",
      "4 5760906 D1=0.867448 D2=1.16471 D3+=1.31979\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 194 assuming -p 1.5\n",
      "probing 218 assuming -r models -p 1.5\n",
      "trie     83 without quantization\n",
      "trie     44 assuming -q 8 -b 8 quantization \n",
      "trie     75 assuming -a 22 array pointer compression\n",
      "trie     37 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105760572 kB\tVmRSS:21292 kB\tRSSMax:21277920 kB\tuser:9.25655\tsys:5.09671\tCPU:14.3533\treal:15.9127\n",
      " \n",
      "Reading fisher_training_utterances_rev_4gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='5', p='0', training_set_prefix='fisher_training_utterances', rev='False'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:10546552832 3:19774787584 4:31639658496 5:46141173760\n",
      "Statistics:\n",
      "1 42458 D1=0.568635 D2=0.994079 D3+=1.51652\n",
      "2 861871 D1=0.701037 D2=1.08009 D3+=1.40893\n",
      "3 3259120 D1=0.797312 D2=1.11694 D3+=1.37379\n",
      "4 5760906 D1=0.880397 D2=1.201 D3+=1.37281\n",
      "5 6927873 D1=0.932391 D2=1.25564 D3+=1.3725\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 346 assuming -p 1.5\n",
      "probing 402 assuming -r models -p 1.5\n",
      "trie    159 without quantization\n",
      "trie     85 assuming -q 8 -b 8 quantization \n",
      "trie    141 assuming -a 22 array pointer compression\n",
      "trie     67 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744 5:193980444\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744 5:193980444\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105767780 kB\tVmRSS:21280 kB\tRSSMax:18854740 kB\tuser:12.6461\tsys:5.78596\tCPU:18.4321\treal:20.5159\n",
      " \n",
      "Reading fisher_training_utterances_5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Building .arpa and .memmap files for n='5', p='0', training_set_prefix='fisher_training_utterances', rev='True'\n",
      "\n",
      "\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/cube/home/AD/emeinhar/fisher-lm/fisher_training_utterances_rev.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 9660450 types 42458\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:509496 2:10546552832 3:19774787584 4:31639658496 5:46141173760\n",
      "Statistics:\n",
      "1 42458 D1=0.562873 D2=1.02199 D3+=1.47276\n",
      "2 861871 D1=0.697381 D2=1.06969 D3+=1.41492\n",
      "3 3259120 D1=0.793798 D2=1.10843 D3+=1.35036\n",
      "4 5760906 D1=0.876797 D2=1.18285 D3+=1.35955\n",
      "5 6927873 D1=0.932391 D2=1.25564 D3+=1.3725\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 346 assuming -p 1.5\n",
      "probing 402 assuming -r models -p 1.5\n",
      "trie    159 without quantization\n",
      "trie     85 assuming -q 8 -b 8 quantization \n",
      "trie    141 assuming -a 22 array pointer compression\n",
      "trie     67 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744 5:193980444\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:509496 2:13789936 3:65182400 4:138261744 5:193980444\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:105767780 kB\tVmRSS:21344 kB\tRSSMax:18860600 kB\tuser:12.7349\tsys:5.89584\tCPU:18.6307\treal:20.7493\n",
      " \n",
      "Reading fisher_training_utterances_rev_5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "\n",
      "\n",
      "Done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peak memory usage is something like 40GB\n",
    "# 2m50s on wittgenstein\n",
    "output_files = []\n",
    "for n, p, prefix, rev in parameter_combinations:\n",
    "    output_files.append(build_model(n, p, prefix, rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.369065Z",
     "start_time": "2019-08-23T20:44:08.364318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_1gram.arpa',\n",
       "  'mmap': None},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_1gram.arpa',\n",
       "  'mmap': None},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_2gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_2gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_3gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_3gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_4gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_4gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_4gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_4gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_5gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_5gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_5gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_5gram.mmap'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.528655Z",
     "start_time": "2019-08-23T20:44:08.370826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_1gram.arpa',\n",
       "   'mmap': None}),\n",
       " ((1, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_1gram.arpa',\n",
       "   'mmap': None}),\n",
       " ((2, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_2gram.mmap'}),\n",
       " ((2, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_2gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_2gram.mmap'}),\n",
       " ((3, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_3gram.mmap'}),\n",
       " ((3, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_3gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_3gram.mmap'}),\n",
       " ((4, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_4gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_4gram.mmap'}),\n",
       " ((4, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_4gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_4gram.mmap'}),\n",
       " ((5, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_5gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_5gram.mmap'}),\n",
       " ((5, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_5gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_5gram.mmap'}))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_and_models = tuple(zip(parameter_combinations,\n",
    "                                  output_files))\n",
    "parameters_and_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and querying a model using the `kenlm` python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.533031Z",
     "start_time": "2019-08-23T20:44:08.530334Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(arpa_or_memmap_fp):\n",
    "    return kenlm.LanguageModel(arpa_or_memmap_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.538847Z",
     "start_time": "2019-08-23T20:44:08.534210Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log2, log10, pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.549164Z",
     "start_time": "2019-08-23T20:44:08.540096Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_utterance_score_and_perplexity_functions(model, base=None, parallelize=False):\n",
    "    if base is None:\n",
    "        base = 10\n",
    "    assert base == 10 or base == 2\n",
    "\n",
    "    if base == 10:\n",
    "        changeOfBase = lambda log10p: log10p\n",
    "    else:\n",
    "        changeOfBase = lambda log10p: log2(pow(10, log10p))\n",
    "    \n",
    "    def score(utterance):\n",
    "        return changeOfBase( model.score(utterance) )\n",
    "    \n",
    "    def perplexity_u(utterance):\n",
    "        score = changeOfBase( model.score(utterance) )\n",
    "        n = len(utterance.split(' ')) + 1\n",
    "#         print('base = {0}'.format(base))\n",
    "#         print('{0} vs. {1}'.format(2 ** (-1.0 * score / n), pow(base, -1.0 * score / n)))\n",
    "        perp = pow(base, -1.0 * score / n)\n",
    "        return perp\n",
    "    \n",
    "    def perplexity_c(utterances):\n",
    "        N = sum(map(lambda utt: len(utt.split(' ')) + 1,\n",
    "                    utterances))\n",
    "        \n",
    "        if not parallelize:\n",
    "            sentence_scores = (score(u) for u in utterances)\n",
    "            sum_of_scores = sum(sentence_scores)\n",
    "        else:\n",
    "            sentence_scores = par((delayed(score)(u) for u in utterances))\n",
    "            sum_of_scores = sum(sentence_scores)\n",
    "        \n",
    "        perp = pow(base, -1.0 * (1.0 / N) * sum_of_scores)\n",
    "        return perp\n",
    "    \n",
    "#     return {'score':score, \n",
    "#             'perplexity':perplexity}\n",
    "    return score, perplexity_u, perplexity_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.574360Z",
     "start_time": "2019-08-23T20:44:08.550442Z"
    }
   },
   "outputs": [],
   "source": [
    "def changeOfBase(log10p):\n",
    "    return log2(pow(10, log10p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.585164Z",
     "start_time": "2019-08-23T20:44:08.576448Z"
    }
   },
   "outputs": [],
   "source": [
    "def perplexity_corpus(utterances, model, base=None, parallelize=False):\n",
    "    if base is None:\n",
    "        base = 10\n",
    "    assert base == 10 or base == 2\n",
    "    \n",
    "    if base == 10:\n",
    "        changeOfBase = lambda log10p: log10p\n",
    "#     else:\n",
    "#         changeOfBase = lambda log10p: log2(pow(10, log10p))\n",
    "#     score = lambda utt: changeOfBase( model.score(utt) )\n",
    "    \n",
    "    N = sum(map(lambda utt: len(utt.split(' ')) + 1,\n",
    "                    utterances))\n",
    "    if not parallelize:\n",
    "        sentence_scores = (changeOfBase(model.score(u)) for u in utterances)\n",
    "        sum_of_scores = sum(sentence_scores)\n",
    "    else:\n",
    "        if base == 10:\n",
    "            sentence_scores = par((delayed(model.score)(u) for u in utterances))\n",
    "            sum_of_scores = sum(sentence_scores)\n",
    "        else:\n",
    "            sentence_scores = par((delayed(model.score)(u) for u in utterances))\n",
    "            sentence_scores_base2 = (changeOfBase(s) for s in sentence_scores)\n",
    "            sum_of_scores = sum(sentence_scores)\n",
    "\n",
    "    perp = pow(base, -1.0 * (1.0 / N) * sum_of_scores)\n",
    "    return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.601172Z",
     "start_time": "2019-08-23T20:44:08.586817Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram = make_model('fisher_training_utterances_2gram.mmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.613661Z",
     "start_time": "2019-08-23T20:44:08.602692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.59227466583252"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.59227466583252"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.59227466583252"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-11.219353675842285"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((-2.6841859817504883, 2, False),\n",
       " (-0.6199646592140198, 2, False),\n",
       " (-1.2637051343917847, 2, False),\n",
       " (-4.348629474639893, 2, False),\n",
       " (-0.6757899522781372, 2, False))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-9.592275202274323"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.score(\"this is a sentence\")\n",
    "bigram.score(\"this is a sentence\", eos = True)\n",
    "bigram.score(\"this is a sentence </s>\", eos=False)\n",
    "bigram.score(\"this is a sentence </s>\")\n",
    "tuple(bigram.full_scores(\"this is a sentence\"))\n",
    "sum(map(lambda triple: triple[0],\n",
    "        tuple(bigram.full_scores(\"this is a sentence\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.617164Z",
     "start_time": "2019-08-23T20:44:08.614574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"has anyone ever told you\"\n",
    "n = len(test_sentence.split(' ')) + 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.622954Z",
     "start_time": "2019-08-23T20:44:08.618500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.431868553161621"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bigram.score(test_sentence)\n",
    "s # = log_10( p(test_sentence) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.629801Z",
     "start_time": "2019-08-23T20:44:08.624210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373.23132982001835"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "373.23132982001835"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.perplexity(test_sentence)\n",
    "10.0 ** (-1.0 * s / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.633493Z",
     "start_time": "2019-08-23T20:44:08.630986Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_score, bigram_perplexity_utt, bigram_perplexity_corpus = make_utterance_score_and_perplexity_functions(bigram, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.642412Z",
     "start_time": "2019-08-23T20:44:08.634687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.431868553161621"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "373.23132982001835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "373.23132982001835"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_score(test_sentence)\n",
    "bigram_perplexity_utt(test_sentence)\n",
    "bigram_perplexity_corpus([test_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.648749Z",
     "start_time": "2019-08-23T20:44:08.643675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903.3458098303903"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [test_sentence, 'call me ishmael']\n",
    "bigram_perplexity_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate perplexity of the test set for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:08.657039Z",
     "start_time": "2019-08-23T20:44:08.649965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_1gram.arpa',\n",
       "   'mmap': None}),\n",
       " ((1, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_1gram.arpa',\n",
       "   'mmap': None}),\n",
       " ((2, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_2gram.mmap'}),\n",
       " ((2, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_2gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_2gram.mmap'}),\n",
       " ((3, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_3gram.mmap'}),\n",
       " ((3, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_3gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_3gram.mmap'}),\n",
       " ((4, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_4gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_4gram.mmap'}),\n",
       " ((4, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_4gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_4gram.mmap'}),\n",
       " ((5, 0, 'fisher_training_utterances', False),\n",
       "  {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_5gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_5gram.mmap'}),\n",
       " ((5, 0, 'fisher_training_utterances', True),\n",
       "  {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_5gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_5gram.mmap'}))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters_and_models = tuple(zip(parameter_combinations,\n",
    "#                                   output_files))\n",
    "parameters_and_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:44:56.751020Z",
     "start_time": "2019-08-23T20:44:56.743092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_1gram.arpa',\n",
       "  'mmap': None},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_1gram.arpa',\n",
       "  'mmap': None},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_2gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_2gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_3gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_3gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_4gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_4gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_4gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_4gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_5gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_5gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_5gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_5gram.mmap'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_2gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_2gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_3gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_3gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_4gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_4gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_4gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_4gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_5gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_5gram.mmap'},\n",
       " {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "  'arpa': 'fisher_training_utterances_rev_5gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_rev_5gram.mmap'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_files\n",
    "' '\n",
    "output_files[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:45:03.267358Z",
     "start_time": "2019-08-23T20:45:03.183551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models = tuple(map(make_model,\n",
    "#                    (output_files[0]['arpa'],) + tuple(map(lambda output_file_dict:output_file_dict['mmap'],\n",
    "#                                                           output_files[1:]))))\n",
    "models = tuple(map(make_model,\n",
    "                   tuple(map(lambda output_file_dict:output_file_dict['mmap'],\n",
    "                             output_files[2:]))))\n",
    "\n",
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:45:24.317517Z",
     "start_time": "2019-08-23T20:45:24.308030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'parameters': (1, 0, 'fisher_training_utterances', False),\n",
       "  'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_1gram.arpa',\n",
       "   'mmap': None},\n",
       "  'model': None},\n",
       " {'parameters': (1, 0, 'fisher_training_utterances', True),\n",
       "  'files': {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_1gram.arpa',\n",
       "   'mmap': None},\n",
       "  'model': None},\n",
       " {'parameters': (2, 0, 'fisher_training_utterances', False),\n",
       "  'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_2gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_2gram.mmap'>},\n",
       " {'parameters': (2, 0, 'fisher_training_utterances', True),\n",
       "  'files': {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_2gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_2gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_rev_2gram.mmap'>},\n",
       " {'parameters': (3, 0, 'fisher_training_utterances', False),\n",
       "  'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_3gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_3gram.mmap'>},\n",
       " {'parameters': (3, 0, 'fisher_training_utterances', True),\n",
       "  'files': {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_3gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_3gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_rev_3gram.mmap'>},\n",
       " {'parameters': (4, 0, 'fisher_training_utterances', False),\n",
       "  'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_4gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_4gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_4gram.mmap'>},\n",
       " {'parameters': (4, 0, 'fisher_training_utterances', True),\n",
       "  'files': {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_4gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_4gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_rev_4gram.mmap'>},\n",
       " {'parameters': (5, 0, 'fisher_training_utterances', False),\n",
       "  'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "   'arpa': 'fisher_training_utterances_5gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_5gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_5gram.mmap'>},\n",
       " {'parameters': (5, 0, 'fisher_training_utterances', True),\n",
       "  'files': {'training_set': 'fisher_training_utterances_rev.txt',\n",
       "   'arpa': 'fisher_training_utterances_rev_5gram.arpa',\n",
       "   'mmap': 'fisher_training_utterances_rev_5gram.mmap'},\n",
       "  'model': <Model from b'fisher_training_utterances_rev_5gram.mmap'>})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_collection = tuple(zip(parameter_combinations,\n",
    "                             output_files,\n",
    "                             (None,None) + models))\n",
    "add_labels = lambda threeTuple: {'parameters':threeTuple[0],\n",
    "                                 'files':threeTuple[1],\n",
    "                                 'model':threeTuple[2]}\n",
    "model_collection = tuple(map(add_labels,\n",
    "                             model_collection))\n",
    "model_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:46:59.137384Z",
     "start_time": "2019-08-23T20:46:59.133005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parameters': (3, 0, 'fisher_training_utterances', False),\n",
       " 'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_3gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_3gram.mmap'},\n",
       " 'model': <Model from b'fisher_training_utterances_3gram.mmap'>}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_collection[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:45:45.753360Z",
     "start_time": "2019-08-23T20:45:45.748390Z"
    }
   },
   "outputs": [],
   "source": [
    "def testModel(model_dict, base, parallelize, corpus):\n",
    "    d = model_dict\n",
    "    params = d['parameters']\n",
    "    n = params[0]\n",
    "    p = params[1]\n",
    "    print('Testing model w/ params n = {0} and p = {1}'.format(n, p))\n",
    "    model = d['model']\n",
    "    \n",
    "#     _, _, perplexity_corpus = make_utterance_score_and_perplexity_functions(model, base, parallelize)\n",
    "    \n",
    "    perp = perplexity_corpus(corpus, model, base, parallelize)\n",
    "    return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:45:49.000878Z",
     "start_time": "2019-08-23T20:45:48.997288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107781"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_set = readStringList\n",
    "len(test_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:47:14.818536Z",
     "start_time": "2019-08-23T20:47:14.814380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parameters': (2, 0, 'fisher_training_utterances', False),\n",
       " 'files': {'training_set': 'fisher_training_utterances.txt',\n",
       "  'arpa': 'fisher_training_utterances_2gram.arpa',\n",
       "  'mmap': 'fisher_training_utterances_2gram.mmap'},\n",
       " 'model': <Model from b'fisher_training_utterances_2gram.mmap'>}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_collection[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:46:11.793917Z",
     "start_time": "2019-08-23T20:46:11.563375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model w/ params n = 2 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.52933814992959"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigram model, base 10 perplexity, parallelize\n",
    "testModel(model_collection[2], 10, False, test_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T20:49:08.181046Z",
     "start_time": "2019-08-23T20:49:05.594769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal (L⟶R) model:\n",
      "Testing model w/ params n = 2 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.52933814992959"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Reversed (L⟵R) model:\n",
      "Testing model w/ params n = 2 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.51589644096235"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Normal (L⟶R) model:\n",
      "Testing model w/ params n = 3 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.82614273676198"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Reversed (L⟵R) model:\n",
      "Testing model w/ params n = 3 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.89188898841859"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Normal (L⟶R) model:\n",
      "Testing model w/ params n = 4 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.79243667473473"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Reversed (L⟵R) model:\n",
      "Testing model w/ params n = 4 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.9357583463501"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Normal (L⟶R) model:\n",
      "Testing model w/ params n = 5 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.68897181326514"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Reversed (L⟵R) model:\n",
      "Testing model w/ params n = 5 and p = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67.87859158005193"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for m in model_collection[2:]:\n",
    "    if m['parameters'][3] == False: #non-reversed strings\n",
    "        print('Normal (L⟶R) model:')\n",
    "        testModel(m, 10, False, test_utterances)\n",
    "    else:\n",
    "        print('Reversed (L⟵R) model:')\n",
    "        testModel(m, 10, False, test_utterances_reversed)\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: try pruning parameters, try an off-the-shelf set of .arpa weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
